{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "Each row in a confusion matrix represents an actual class, while each column represents a predicted class.\n",
    "<img src=\"imgs/confusion matrix.png\" alt=\"confusion matrix\" style=\"width: 230px;\"/>\n",
    "\n",
    "__Precision__ of the classifier is the accuracy of the positive prediction. It expresses the proportion of the instances that our model says was positive were actually positive:\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "\n",
    "__Recall__, also called _sensitivity_ or _true positive rate_, is the ratio of positive instances that are correctly detected by the classifier. It represents the ability to find all positive instances in the dataset:\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "__F1 Score__ is the harmonic mean of precision and recall. The classifier will only get high F1 score if both recall and precision are high:\n",
    "$$F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = \\frac{TP}{TP + \\frac{FN + FP}{2}}$$\n",
    "\n",
    "> If someone says \"let's reach 99% precision,\" you should ask,\"at what recall?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The ROC Curve\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curve is another common tool used with binary classifiers. It plots the __true positive rate (recall)__ against the __false positive rate__ (FPR, ratio of negative instances that are incorrectly classified as positive).\n",
    "\n",
    "One way to compare classifiers is to measure the area under the curve (AUC)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,method=\"decision_function\")\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None): \n",
    "        plt.plot(fpr, tpr, linewidth=2, label=label) \n",
    "        plt.plot([0, 1], [0, 1], 'k--') plt.axis([0, 1, 0, 1])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "> As a rule of thumb, you should prefer the (precision / recall) PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "_Stochastic gradient descent_ will end up very close to the minimum, but once it gets there it will continue to bounce around and never settling down. So when the cost function is very irregular, it has a better chance of finding the global minimum than _batch gradient descent_ does.\n",
    "\n",
    "### The Bias / Variance Tradeoff\n",
    "\n",
    "Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model’s complexity increases its bias and reduces its variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree\n",
    "\n",
    "__Do not require _feature scaling or centering_ at all__. It makes very few assumptions about the training data.\n",
    "\n",
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train Decision Trees (Training Algorithm). The algorithm first splits the training set in two subsets using a single feature $k$ and a threshold $t_k$. To choose $k$ and $k_t$, it searches for the pair ($k$, $t_k$) that produces the purest subsets. The training algorithm in Scikit-Learn is stochastic, so it randomly selects the set of features instead of using all features to evaluate at each node.\n",
    "\n",
    "__Training complexity__: $O(n\\times m\\log(m))$, since you need to compare all features on all samples at each node.\n",
    "\n",
    "__Prediction complexity__: $O(\\log_2(m))$, which is independent of the number of features. Making predictions requires traversing the Decision Tree from the root to a leaf\n",
    "\n",
    "### Gini Impurity or Entropy?\n",
    "Gini impurity is slightly faster and tends to isolate the most frequent class in its own branch of the tree.\n",
    "Entropy tends to produce slightly more balanced trees.\n",
    "\n",
    "### Regularization Hyperparameters\n",
    "\n",
    "- _max-depth_: maximum depth of the Decision Tree. Default, None.\n",
    "\n",
    "- _min-samples-split_: the minimum number of samples a node must have before it can be split.\n",
    "\n",
    "- _min-samples-leaf_: the minimum number of samples a leaf node must have.\n",
    "\n",
    "- _min-weight-fraction-leaf_: same as _min-samples-leaf_ but expressed as a fraction of the total number of weighted instances).\n",
    "\n",
    "- _max-leaf-nodes_: maximum number of leaf nodes.\n",
    "\n",
    "- _max-features_: maximum number of features that are evaluated for splitting at each node\n",
    "\n",
    "Increasing __min\\*__ hyperparameters or reducing __max\\*__ will regularize the model\n",
    "\n",
    "\n",
    "### Instability\n",
    "\n",
    "1. Orthogonal decision boundaries (all splits are perpendicular to an axis): makes them sensitive to training set rotation. One way to limit this problem is to use PCA, which often results in a better orientation of the training data.\n",
    "1. Sensitive to small variations in the training data. Since the training algorithm used by Scikit-Learn is stochastic, you may even get very different models on the same training data. Random Forests can limit this instability by averaging predictions over many trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede‐ cessor.\n",
    "\n",
    "There are two ways to get a diverse set of classifiers:\n",
    "1. use very different training algorithms\n",
    "1. use the same training algorithm but to train them on different random subsets of the training set. __Bagging__ means sampling with replacement. __Pasting__ is sampling without replacement.\n",
    "\n",
    "__hard voting classifier__: Aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "__soft voting classifier__: If all classifiers are able to estimate class probabilities, predict the class with the highest class probability, averaged over all the individual classifiers.\n",
    "\n",
    "__Boosting__: any ensemble method that can combine several weak learners into a strong learner.\n",
    "- Adaboost: sequential training with instance weight updates.\n",
    "- Gradient Boosting: fitting the new predictor to the residual errors made by the previous predictor.\n",
    "- stochastic Gradient Boosting: training on xx% of the training instances selected randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dinensionality Reduction\n",
    "\n",
    "__Curse of Dimensinality__: with the increasing of the number of features for each training instances, the training becomes extremely slow and it's much harder to find a good solution. The hight-dimensional datasets are at risk of being very sparse. The more dimensions the training set has, the greater the risk of overfitting. The number of training instances required grows exponentially with the number of dimensions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
